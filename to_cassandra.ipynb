{
 "cells": [
  {
   "cell_type": "code",
   "id": "0338521a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T01:15:10.939782Z",
     "start_time": "2025-04-04T01:15:09.483563Z"
    }
   },
   "source": [
    "!pip list | grep spark"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark                   2.0.1\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7209eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb256baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/04 01:46:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e935d185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/04 01:46:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraTest\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "225e6a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n",
      "|                  id|age| name|\n",
      "+--------------------+---+-----+\n",
      "|32520ada-b1af-4c5...| 25|Alice|\n",
      "+--------------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"test_keyspace\") \\\n",
    "    .option(\"table\", \"test_table\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f36b52b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/04 02:06:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------+---------+--------+-----------------+---------+-------------+------------+------------+----------------+---------+--------+----+--------+--------+-------+-------------------+---------+----------+-----------------+------+--------------+-------+--------+-------------+----------+---------+\n",
      "|flight_id|actual_elapsed_time|air_time|arr_delay|arr_time|cancellation_code|cancelled|carrier_delay|crs_arr_time|crs_dep_time|crs_elapsed_time|dep_delay|dep_time|dest|distance|diverted|fl_date|late_aircraft_delay|nas_delay|op_carrier|op_carrier_fl_num|origin|security_delay|taxi_in|taxi_out|weather_delay|wheels_off|wheels_on|\n",
      "+---------+-------------------+--------+---------+--------+-----------------+---------+-------------+------------+------------+----------------+---------+--------+----+--------+--------+-------+-------------------+---------+----------+-----------------+------+--------------+-------+--------+-------------+----------+---------+\n",
      "+---------+-------------------+--------+---------+--------+-----------------+---------+-------------+------------+------------+----------------+---------+--------+----+--------+--------+-------+-------------------+---------+----------+-----------------+------+--------------+-------+--------+-------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"airline_data\") \\\n",
    "    .option(\"table\", \"flights\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23775e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=================================================>      (80 + 8) / 90]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import FloatType, BooleanType, StringType\n",
    "import uuid\n",
    "\n",
    "# Load CSV data\n",
    "csv_path = \"data/airline.csv.shuffle\"  # Change to actual file path\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True).limit(10)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define UUID generator for primary key\n",
    "def generate_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "uuid_udf = udf(generate_uuid, StringType())\n",
    "\n",
    "# Add UUID column\n",
    "df = df.withColumn(\"flight_id\", uuid_udf())\n",
    "\n",
    "# Convert column names to match Cassandra schema\n",
    "df = df.select(\n",
    "    col(\"flight_id\"),\n",
    "    col(\"FL_DATE\").alias(\"fl_date\"),\n",
    "    col(\"OP_CARRIER\").alias(\"op_carrier\"),\n",
    "    col(\"OP_CARRIER_FL_NUM\").alias(\"op_carrier_fl_num\"),\n",
    "    col(\"ORIGIN\").alias(\"origin\"),\n",
    "    col(\"DEST\").alias(\"dest\"),\n",
    "    col(\"CRS_DEP_TIME\").alias(\"crs_dep_time\"),\n",
    "    col(\"DEP_TIME\").alias(\"dep_time\"),\n",
    "    col(\"DEP_DELAY\").cast(FloatType()).alias(\"dep_delay\"),\n",
    "    col(\"TAXI_OUT\").cast(FloatType()).alias(\"taxi_out\"),\n",
    "    col(\"WHEELS_OFF\").alias(\"wheels_off\"),\n",
    "    col(\"WHEELS_ON\").alias(\"wheels_on\"),\n",
    "    col(\"TAXI_IN\").cast(FloatType()).alias(\"taxi_in\"),\n",
    "    col(\"CRS_ARR_TIME\").alias(\"crs_arr_time\"),\n",
    "    col(\"ARR_TIME\").alias(\"arr_time\"),\n",
    "    col(\"ARR_DELAY\").cast(FloatType()).alias(\"arr_delay\"),\n",
    "    col(\"CANCELLED\").cast(BooleanType()).alias(\"cancelled\"),\n",
    "    col(\"CANCELLATION_CODE\").alias(\"cancellation_code\"),\n",
    "    col(\"DIVERTED\").cast(BooleanType()).alias(\"diverted\"),\n",
    "    col(\"CRS_ELAPSED_TIME\").cast(FloatType()).alias(\"crs_elapsed_time\"),\n",
    "    col(\"ACTUAL_ELAPSED_TIME\").cast(FloatType()).alias(\"actual_elapsed_time\"),\n",
    "    col(\"AIR_TIME\").cast(FloatType()).alias(\"air_time\"),\n",
    "    col(\"DISTANCE\").cast(FloatType()).alias(\"distance\"),\n",
    "    col(\"CARRIER_DELAY\").cast(FloatType()).alias(\"carrier_delay\"),\n",
    "    col(\"WEATHER_DELAY\").cast(FloatType()).alias(\"weather_delay\"),\n",
    "    col(\"NAS_DELAY\").cast(FloatType()).alias(\"nas_delay\"),\n",
    "    col(\"SECURITY_DELAY\").cast(FloatType()).alias(\"security_delay\"),\n",
    "    col(\"LATE_AIRCRAFT_DELAY\").cast(FloatType()).alias(\"late_aircraft_delay\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Cassandra\n",
    "df.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .options(table=\"flights\", keyspace=\"airline_data\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data successfully written to Cassandra!\")\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529db3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53cf63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf4b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
